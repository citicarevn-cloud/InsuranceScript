import streamlit as st
import google.generativeai as genai
from gtts import gTTS
import tempfile
import os
import requests
import re
import time
import random
import asyncio
import edge_tts
import concurrent.futures
from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips, ColorClip

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="DAT Media AI Studio", layout="wide", page_icon="üéôÔ∏è")

# --- CSS ---
st.markdown("""
    <style>
    .stButton>button {background-color: #0068C9; color: white; font-weight: bold; border-radius: 8px; height: 3em; width: 100%;}
    img {border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); margin: 10px 0;}
    .stProgress > div > div > div > div { background-color: #28a745; }
    </style>
""", unsafe_allow_html=True)

# --- QU·∫¢N L√ù SESSION ---
if 'feedback_history' not in st.session_state: st.session_state.feedback_history = []
if 'video_settings' not in st.session_state: st.session_state.video_settings = {'w': 1280, 'h': 720}

# --- C·∫§U H√åNH VOICE ID ---
VOICE_MAP = {
    "Chuy√™n nghi·ªáp": "mJLZ5p8I7Pk81BHpKwbx",  # Nam Sadoma
    "ƒê·ªùi th∆∞·ªùng": "foH7s9fX31wFFH2yqrFa",     # Huyen
    "C·∫£m ƒë·ªông": "1l0C0QA9c9jN22EmWiB0",       # Jade
    "H√†i h∆∞·ªõc": "JxmKvRaNYFidf0N27Vng"        # Son Tran
}

# --- SIDEBAR ---
with st.sidebar:
    st.header("üéõÔ∏è C·∫•u h√¨nh h·ªá th·ªëng")
    
    if st.button("üîÑ L√ÄM M·ªöI (RESET)"):
        st.session_state.clear()
        st.rerun()

    # 1. NH·∫¨P KEY
    api_key_raw = st.secrets.get("GEMINI_API_KEY", "")
    eleven_api_raw = st.secrets.get("ELEVEN_API_KEY", "")
    hf_token_raw = st.secrets.get("HUGGINGFACE_TOKEN", "")

    if not api_key_raw: api_key_raw = st.text_input("Gemini API Key", type="password")
    if not eleven_api_raw: eleven_api_raw = st.text_input("ElevenLabs API Key", type="password")
    if not hf_token_raw: hf_token_raw = st.text_input("HuggingFace Token (B·∫ÆT BU·ªòC)", type="password")
    
    # Clean Keys
    api_key = api_key_raw.strip() if api_key_raw else ""
    eleven_api = eleven_api_raw.strip() if eleven_api_raw else ""
    hf_token = hf_token_raw.strip() if hf_token_raw else ""

    if api_key: st.success("‚úÖ Gemini: OK")
    if eleven_api: st.success("‚úÖ ElevenLabs: OK")
    if hf_token: st.success("‚úÖ HuggingFace: OK")
    else: st.error("‚ùå Thi·∫øu Token HuggingFace")

    st.divider()
    
    # 2. MODEL GEMINI
    st.subheader("üß† B·ªô n√£o x·ª≠ l√Ω")
    available_models = ["models/gemini-pro"]
    if api_key:
        try:
            genai.configure(api_key=api_key)
            models = genai.list_models()
            available_models = [m.name for m in models if 'generateContent' in m.supported_generation_methods]
        except: pass
    selected_model = st.selectbox("Ch·ªçn Model:", available_models, index=0)

    st.divider()

    # 3. GI·ªåNG ƒê·ªåC
    st.subheader("üîä Ngu·ªìn gi·ªçng ƒë·ªçc")
    tts_provider = st.selectbox("Ch·ªçn Server:", ["ElevenLabs (VIP)", "Microsoft (Mi·ªÖn ph√≠)"])
    
    edge_voice = "vi-VN-HoaiMyNeural" 
    if "Microsoft" in tts_provider:
        edge_voice = st.selectbox("Ch·ªçn gi·ªçng MS:", [
            "vi-VN-HoaiMyNeural (N·ªØ)", "vi-VN-NamMinhNeural (Nam)"
        ]).split(" ")[0]

# --- H√ÄM X·ª¨ L√ù (CORE) ---

def clean_text_for_audio(text):
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)
    prefixes = ["L·ªùi b√¨nh:", "Audio:", "Voice:", "Thuy·∫øt minh:", "Host:", "MC:", "Scene \d+:"]
    for p in prefixes:
        text = re.sub(f'{p}', '', text, flags=re.IGNORECASE)
    text = text.replace("*", "").replace("#", "").replace("- ", "").replace('"', '')
    return text.strip()

async def generate_edge_tts(text, voice, filename):
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(filename)

def generate_audio_strict(text, filename, tone_key="Chuy√™n nghi·ªáp"):
    clean_text = clean_text_for_audio(text)
    if not clean_text: return False
    
    # 1. ELEVENLABS STRICT MODE
    if "ElevenLabs" in tts_provider:
        if not eleven_api:
            st.error("‚ùå L·ªói: B·∫°n ch·ªçn ElevenLabs nh∆∞ng ch∆∞a nh·∫≠p API Key!")
            return False
            
        voice_id = VOICE_MAP.get(tone_key, "mJLZ5p8I7Pk81BHpKwbx").strip()
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
        headers = {"xi-api-key": eleven_api, "Content-Type": "application/json"}
        # T·ª± ƒë·ªông ch·ªçn model ·ªïn ƒë·ªãnh
        data = {"text": clean_text, "model_id": "eleven_multilingual_v2"} 
        
        try:
            response = requests.post(url, json=data, headers=headers, timeout=60)
            if response.status_code == 200:
                with open(filename, 'wb') as f: f.write(response.content)
                return True
            else:
                st.error(f"‚ùå ElevenLabs t·ª´ ch·ªëi: {response.status_code}")
                st.code(response.text)
                return False 
        except Exception as e:
            st.error(f"‚ùå L·ªói k·∫øt n·ªëi ElevenLabs: {e}")
            return False

    # 2. MICROSOFT STRICT MODE
    if "Microsoft" in tts_provider:
        try:
            asyncio.run(generate_edge_tts(clean_text, edge_voice, filename))
            return True
        except Exception as e:
            st.error(f"‚ùå L·ªói Microsoft TTS: {e}")
            return False

    return False

def generate_image_huggingface_only(prompt, width, height):
    if not hf_token:
        st.error("‚ùå Ch∆∞a c√≥ Token Hugging Face.")
        return None

    API_URL = "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0"
    headers = {"Authorization": f"Bearer {hf_token}"}
    
    style = ", high quality illustration, isometric style, flat design, cinematic lighting, no text"
    full_prompt = prompt + style
    if width < height: full_prompt += ", vertical, 9:16 portrait"
    else: full_prompt += ", wide angle, 16:9 landscape"

    for attempt in range(3):
        try:
            response = requests.post(API_URL, headers=headers, json={"inputs": full_prompt}, timeout=20)
            if response.status_code == 200:
                return response.content
            else:
                err_info = response.json()
                if 'estimated_time' in err_info:
                    wait_time = err_info['estimated_time']
                    st.toast(f"‚è≥ Model ƒëang kh·ªüi ƒë·ªông, ƒë·ª£i {wait_time:.1f}s...")
                    time.sleep(wait_time + 1)
                else:
                    return None
        except: time.sleep(1)
            
    return None

def process_scene_strict(args):
    part, width, height, tone = args
    try:
        if "|" in part:
            data = part.split("|")
            if len(data) < 2: return None
            
            img_prompt = data[0].replace("Scene", "").replace(":", "").strip()
            raw_voice_text = data[1].strip()
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as f:
                audio_path = f.name
            
            success = generate_audio_strict(raw_voice_text, audio_path, tone)
            
            if not success: return None

            img_content = generate_image_huggingface_only(img_prompt, width, height)
            
            if img_content:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as f:
                    f.write(img_content); img_path = f.name
                return (audio_path, img_path)
            else:
                return (audio_path, "PLACEHOLDER")
    except: return None

def create_video_strict(script_data, width, height, tone):
    lines = [line for line in script_data.strip().split('\n') if "|" in line and "Scene" in line]
    if len(lines) > 10: lines = lines[:10]
    total_scenes = len(lines)
    if total_scenes == 0: return None

    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text(f"üöÄ ƒêang x·ª≠ l√Ω Pillar & Angle...")
    
    process_args = [(line, width, height, tone) for line in lines]
    
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        for i, result in enumerate(executor.map(process_scene_strict, process_args)):
            results.append(result)
            progress_bar.progress(int((i + 1) / total_scenes * 100))
            
    status_text.text("üé¨ ƒêang render video...")
    clips = []
    for asset in results:
        if asset:
            audio_path, img_path = asset
            try:
                ac = AudioFileClip(audio_path)
                if img_path == "PLACEHOLDER":
                    clip = ColorClip(size=(width, height), color=(0,0,0), duration=ac.duration + 0.5)
                else:
                    clip = ImageClip(img_path).set_duration(ac.duration + 0.5)
                clip = clip.set_audio(ac).set_fps(15)
                clips.append(clip)
            except: pass

    if clips:
        try:
            final = concatenate_videoclips(clips, method="compose")
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as f:
                final.write_videofile(f.name, codec='libx264', audio_codec='aac', fps=15, preset='ultrafast', threads=4)
            status_text.text("‚úÖ Xong!")
            progress_bar.empty()
            return f.name
        except: return None
    return None

def render_mixed_content(text, width=800, height=450):
    pattern = r'\{{1,2}IMAGE:?\s*(.*?)\}{1,2}'
    parts = re.split(pattern, text, flags=re.IGNORECASE)
    for i, part in enumerate(parts):
        if i % 2 == 0:
            if part.strip(): st.markdown(part)
        else:
            img_prompt = part.strip().replace("}", "").replace("{", "")
            if img_prompt:
                img_content = generate_image_huggingface_only(img_prompt, width, height)
                if img_content:
                    st.image(img_content, caption=f"üé® {img_prompt}", use_container_width=True)
                else:
                    st.warning("‚ö†Ô∏è L·ªói t·∫£i ·∫£nh (HF)")

# --- GIAO DI·ªÜN CH√çNH (N√ÇNG C·∫§P PILLAR & ANGLE) ---
st.title("üõ°Ô∏è DAT Media AI Studio: Content Architect")

col1, col2 = st.columns([1, 1.5], gap="large")

with col1:
    st.subheader("1. Ki·∫øn tr√∫c n·ªôi dung")
    
    # [N√ÇNG C·∫§P 1] TH√äM PILLAR & ANGLE
    col_p1, col_p2 = st.columns(2)
    with col_p1:
        pillar = st.selectbox("1. Nh√≥m ch·ªß ƒë·ªÅ (Pillar)", [
            "Ki·∫øn th·ª©c & Gi√°o d·ª•c (Educate)",
            "S·∫£n ph·∫©m & Gi·∫£i ph√°p (Product)", 
            "Ni·ªÅm tin & B·∫±ng ch·ª©ng (Trust)",
            "Phong c√°ch s·ªëng & C·∫£m x√∫c (Lifestyle)"
        ])
    with col_p2:
        angle = st.selectbox("2. G√≥c ti·∫øp c·∫≠n (Angle)", [
            "Chuy√™n gia ph√¢n t√≠ch (Logic)",
            "K·ªÉ chuy·ªán (Storytelling)",
            "C·∫£nh b√°o/B√≥c tr·∫ßn (Drama)",
            "H√†i h∆∞·ªõc/ƒêu Trend (Fun)",
            "Q&A Gi·∫£i ƒë√°p (Helpful)"
        ])

    keyword = st.text_input("3. T·ª´ kh√≥a c·ª• th·ªÉ", "B·∫£o hi·ªÉm nh√¢n th·ªç cho ng∆∞·ªùi tr·ª• c·ªôt")
    
    content_type = st.radio("4. ƒê·ªãnh d·∫°ng", ["Clip (Video)", "B√†i Website", "B√†i Facebook"])
    
    seo_guide = ""
    video_w, video_h = 1280, 720
    
    if content_type == "Clip (Video)":
        orientation = st.radio("Khung h√¨nh:", ["Ngang 16:9", "D·ªçc 9:16"], horizontal=True)
        if "Ngang" in orientation:
            video_w, video_h = 1280, 720; ratio_txt = "Wide 16:9"
        else:
            video_w, video_h = 720, 1280; ratio_txt = "Vertical 9:16"

        vid_len = st.radio("ƒê·ªô d√†i:", ["Clip Ng·∫Øn (<90s)", "Video D√†i (Preview)"], horizontal=True)
        if "Ng·∫Øn" in vid_len: dur = st.slider("Gi√¢y", 15, 90, 60); dur_txt = f"{dur} gi√¢y"
        else: dur = st.slider("Ph√∫t", 2, 20, 5); dur_txt = f"{dur} ph√∫t"

        seo_guide = f"""
        - Vi·∫øt K·ªãch b·∫£n Video ({ratio_txt}) d√†i {dur_txt}.
        - Phong c√°ch: {angle}. Tr·ª• c·ªôt: {pillar}.
        - ƒê·ªãnh d·∫°ng B·∫ÆT BU·ªòC t·ª´ng d√≤ng: 'Scene X: [M√¥ t·∫£ ·∫£nh ti·∫øng Anh] | [L·ªùi b√¨nh ti·∫øng Vi·ªát]'.
        """
    elif content_type == "B√†i Website":
        words = st.number_input("S·ªë t·ª´", 500, 2500, 1000)
        seo_guide = f"- Vi·∫øt b√†i Website ({pillar}) chu·∫©n SEO {words} t·ª´. G√≥c nh√¨n: {angle}. D√πng th·∫ª {{IMAGE: english prompt}} xen k·∫Ω."
    else:
        seo_guide = f"- Vi·∫øt Caption Facebook ({angle}) cho ch·ªß ƒë·ªÅ {pillar}. ƒê·ªÅ xu·∫•t ·∫£nh vu√¥ng."

    # T·ª± ƒë·ªông map Tone gi·ªçng theo Angle ƒë·ªÉ ch·ªçn Voice cho chu·∫©n
    tone_map = {
        "Chuy√™n gia ph√¢n t√≠ch (Logic)": "Chuy√™n nghi·ªáp",
        "K·ªÉ chuy·ªán (Storytelling)": "C·∫£m ƒë·ªông",
        "C·∫£nh b√°o/B√≥c tr·∫ßn (Drama)": "Chuy√™n nghi·ªáp",
        "H√†i h∆∞·ªõc/ƒêu Trend (Fun)": "H√†i h∆∞·ªõc",
        "Q&A Gi·∫£i ƒë√°p (Helpful)": "ƒê·ªùi th∆∞·ªùng"
    }
    tone_auto = tone_map.get(angle, "Chuy√™n nghi·ªáp")
    st.info(f"üéôÔ∏è Tone gi·ªçng AI ƒë·ªÅ xu·∫•t: **{tone_auto}** (B·∫°n c√≥ th·ªÉ ƒë·ªïi ·ªü Sidebar n·∫øu mu·ªën)")
    
    btn_run = st.button("üöÄ X·ª¨ L√ù NGAY")

# --- K·∫æT QU·∫¢ ---
with col2:
    st.subheader("2. K·∫øt qu·∫£")
    if btn_run:
        error = False
        if not api_key: st.error("Thi·∫øu Gemini Key"); error=True
        if content_type != "B√†i Facebook" and not hf_token: st.error("Thi·∫øu HuggingFace Token"); error=True
        if "ElevenLabs" in tts_provider and not eleven_api: st.error("Thi·∫øu ElevenLabs Key"); error=True

        if not error:
            with st.spinner(f"AI ƒëang s√°ng t·∫°o theo Pillar & Angle..."):
                try:
                    st.session_state.video_settings = {'w': video_w, 'h': video_h}
                    st.session_state.tone_key = tone_auto # D√πng tone t·ª± ƒë·ªông map theo Angle
                    
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel(selected_model) 
                    
                    past_fb = "\n".join([f"- {fb}" for fb in st.session_state.feedback_history])
                    
                    # PROMPT N√ÇNG C·∫§P (QUAN TR·ªåNG)
                    prompt = f"""
                    Vai tr√≤: Chuy√™n gia Content Marketing ng√†nh B·∫£o Hi·ªÉm.
                    Nhi·ªám v·ª•: T·∫°o n·ªôi dung theo ki·∫øn tr√∫c Pillar & Angle.
                    
                    1. INPUT:
                       - Ch·ªß ƒë·ªÅ (Topic): {keyword}
                       - Tr·ª• c·ªôt (Pillar): {pillar} (H√£y b√°m s√°t ƒë·ªãnh h∆∞·ªõng n√†y)
                       - G√≥c ƒë·ªô (Angle): {angle} (H√£y d√πng gi·ªçng vƒÉn v√† c·∫•u tr√∫c n√†y)
                    
                    2. Y√äU C·∫¶U ƒê·∫¶U RA:
                       - TI√äU ƒê·ªÄ CHU·∫®N SEO (Gi·∫≠t t√≠t theo Angle {angle})
                       - 5 HASHTAGS & 5 TAGS
                       - N·ªòI DUNG CH√çNH:
                         {seo_guide}
                    
                    L∆ØU √ù: Kh√¥ng d√πng d·∫•u ** trong l·ªùi b√¨nh video.
                    L∆ØU √ù USER: {past_fb}
                    """
                    response = model.generate_content(prompt)
                    st.session_state.result = response.text
                    st.session_state.type = content_type
                    st.session_state.kw = keyword
                    st.success("ƒê√£ c√≥ n·ªôi dung!")
                except Exception as e: st.error(f"L·ªói Gemini: {e}")

    if 'result' in st.session_state:
        if st.session_state.type == "B√†i Website":
            st.info("üñºÔ∏è ·∫¢nh Featured")
            img_content = generate_image_huggingface_only(f"{st.session_state.kw} insurance header", 1200, 628)
            if img_content: st.image(img_content, use_container_width=True)
            render_mixed_content(st.session_state.result)
        elif st.session_state.type == "B√†i Facebook":
            st.info("üì± ·∫¢nh Vu√¥ng")
            img_content = generate_image_huggingface_only(f"{st.session_state.kw} flat lay", 1080, 1080)
            if img_content: st.image(img_content, width=450)
            st.markdown(st.session_state.result)
        else:
            tab1, tab2 = st.tabs(["üé¨ Video Demo", "üìù K·ªãch b·∫£n"])
            with tab1:
                vw = st.session_state.video_settings['w']
                vh = st.session_state.video_settings['h']
                tk = st.session_state.get('tone_key', "Chuy√™n nghi·ªáp")
                
                if "ElevenLabs" in tts_provider:
                    current_id = VOICE_MAP.get(tk, "").strip()
                    st.info(f"üéôÔ∏è Voice ID: `{current_id}` (Theo Angle: {tk})")
                
                if st.button("üé• D·ª±ng Video"):
                    v_path = create_video_strict(st.session_state.result, vw, vh, tk)
                    if v_path: st.video(v_path)
            with tab2:
                st.text_area("Script", st.session_state.result, height=500)
